{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connecting to Federated Query Executed System\n",
    "\n",
    "### Presto\n",
    "``\n",
    "conn = presto.Connection(host=\"presto\", port=8080, user=\"demo\")\n",
    "cur = conn.cursor()\n",
    "``\n",
    "### Drill\n",
    "``drill = PyDrill(host='drill', port=8047)``\n",
    "### Spark\n",
    "TODO\n",
    "### Hive\n",
    "TODO\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pathlib in /opt/conda/lib/python3.9/site-packages (1.0.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.9/site-packages (2.11.3)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /opt/conda/lib/python3.9/site-packages (from jinja2) (1.1.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install pathlib\n",
    "%pip install jinja2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import jinja2\n",
    "import prestodb.dbapi as presto\n",
    "from pydrill.client import PyDrill\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configure sources\n",
    "sources={\n",
    "    \"li_source\":\"pg1\",\n",
    "    \"o_source\":\"pg2\",\n",
    "    \"c_source\":\"pg3\",\n",
    "    \"pa_source\":\"pg4\",\n",
    "    \"s_source\":\"pg1\",\n",
    "    \"ps_source\":\"pg2\",\n",
    "    \"n_source\":\"pg3\",\n",
    "    \"r_source\":\"pg4\",\n",
    "}\n",
    "\n",
    "def get_sources_config(scale,**kwargs):\n",
    "    \"\"\"Returns a dict with tabels maped to sources\"\"\"\n",
    "    mapped = {\n",
    "        \"lineitem\":f\"{kwargs['li_source']}.public.pg_{scale}_lineitem\",\n",
    "        \"orders\":f\"{kwargs['o_source']}.public.pg_{scale}_orders\",\n",
    "        \"customer\":f\"{kwargs['c_source']}.public.pg_{scale}_customer\",\n",
    "        \"part\":f\"{kwargs['pa_source']}.public.pg_{scale}_part\",\n",
    "        \"supplier\":f\"{kwargs['s_source']}.public.pg_{scale}_supplier\",\n",
    "        \"partsupp\":f\"{kwargs['ps_source']}.public.pg_{scale}_partsupp\",\n",
    "        \"nation\":f\"{kwargs['n_source']}.public.pg_{scale}_nation\",\n",
    "        \"region\":f\"{kwargs['r_source']}.public.pg_{scale}_region\"\n",
    "    }\n",
    "    return mapped\n",
    "\n",
    "\n",
    "config = get_sources_config(scale=\"sf1\",**sources)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_queries():\n",
    "    return [q for q in Path(\"./queries\").iterdir()]\n",
    "\n",
    "def render_queries(sources_config,queries_path):\n",
    "    \"\"\"Returns a dict where the key is the tpch query name and the value is the rendered query\"\"\"\n",
    "    result = {}\n",
    "    for path in queries_path:\n",
    "        key = path.name.split(\".\")[0]\n",
    "        value = jinja2.Template(queries_path[0].read_text()).render(**sources_config)\n",
    "        result[key]=value\n",
    "    return result\n",
    "\n",
    "query_dict = render_queries(sources_config=config,queries_path=get_queries())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "drill = PyDrill(host='drill', port=8047)\n",
    "\n",
    "if not drill.is_active():\n",
    "    raise ValueError('Please run Drill first')\n",
    "\n",
    "conn = presto.Connection(host=\"presto\", port=8080, user=\"demo\")\n",
    "cur = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get catalogs\n",
    "cur.execute(\"SHOW catalogs\")\n",
    "cur.fetchall()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How Does Presto Execute a Query?\n",
    "\n",
    "If you are curious about what Presto translate a SQL query to and what it will run, you can you `EXPLAIN`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "conn = presto.Connection(host=\"presto\", port=8080, user=\"demo\")\n",
    "cur = conn.cursor()\n",
    "cur.execute('''\n",
    "SELECT\n",
    "    l_orderkey,\n",
    "    sum(l_extendedprice * (1 - l_discount)) as revenue,\n",
    "    o_orderdate,\n",
    "    o_shippriority\n",
    "FROM\n",
    "    pg2.public.pg_sf1_customer,\n",
    "    pg3.public.pg_sf1_orders,\n",
    "    pg3.public.pg_sf1_lineitem\n",
    "WHERE\n",
    "    c_mktsegment = 'BUILDING'\n",
    "    AND c_custkey = o_custkey\n",
    "    AND l_orderkey = o_orderkey\n",
    "    AND o_orderdate < date '1995-03-15'\n",
    "    AND l_shipdate > date '1995-03-15'\n",
    "GROUP BY\n",
    "    l_orderkey,\n",
    "    o_orderdate,\n",
    "    o_shippriority\n",
    "ORDER BY\n",
    "    revenue desc,\n",
    "    o_orderdate\n",
    "LIMIT 10\n",
    "''')\n",
    "presto_res = cur.fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "presto_df = pd.DataFrame(presto_res,columns=[\"l_orderkey\",\"revenue\",\"o_orderdate\",\"o_shippriority\"])\n",
    "presto_df[\"l_orderkey\"]=presto_df[\"l_orderkey\"].astype(int)\n",
    "presto_df[\"o_shippriority\"]=presto_df[\"o_shippriority\"].astype(int)\n",
    "presto_df[\"revenue\"]=presto_df[\"revenue\"].astype(float)\n",
    "presto_df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute `docker exec drill /bin/sh -c \"./create_plugins.sh\"` to add storage to drill. This should be done everytime you restart drill container, as there is no persistant storage where the config is saved. \n",
    "\n",
    "This can be improved by finding where the configuration are written ang mounting a volume there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drill_res = drill.query('''SELECT\n",
    "    l_orderkey,\n",
    "    sum(l_extendedprice * (1 - l_discount)) as revenue,\n",
    "    o_orderdate,\n",
    "    o_shippriority\n",
    "FROM\n",
    "    pg2.public.pg_sf1_customer,\n",
    "    pg3.public.pg_sf1_orders,\n",
    "    pg3.public.pg_sf1_lineitem\n",
    "WHERE\n",
    "    c_mktsegment = 'BUILDING'\n",
    "    AND c_custkey = o_custkey\n",
    "    AND l_orderkey = o_orderkey\n",
    "    AND o_orderdate < date '1995-03-15'\n",
    "    AND l_shipdate > date '1995-03-15'\n",
    "GROUP BY\n",
    "    l_orderkey,\n",
    "    o_orderdate,\n",
    "    o_shippriority\n",
    "ORDER BY\n",
    "    revenue desc,\n",
    "    o_orderdate\n",
    "LIMIT 10''', timeout=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drill_df = drill_res.to_dataframe()\n",
    "drill_df[\"l_orderkey\"]=presto_df[\"l_orderkey\"].astype(int)\n",
    "drill_df[\"o_shippriority\"]=presto_df[\"o_shippriority\"].astype(int)\n",
    "drill_df[\"revenue\"]=presto_df[\"revenue\"].astype(float)\n",
    "drill_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def are_equal(df1,df2):\n",
    "    m = df1.merge(df2, how='outer', indicator=True)\n",
    "    return len(m[m[\"_merge\"]!=\"both\"]) == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "are_equal(drill_df,presto_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22.688865184783936\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# def benchamrk(system,query)\n",
    "\n",
    "# start = time.time()\n",
    "# drill.query(query_dict[\"q1\"],timeout=10000)\n",
    "# end = time.time()\n",
    "# print(end - start)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Table in MySQL\n",
    "\n",
    "Let's use the MySQL client to create the table. Then we will switch to Presto to manipulate the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import MySQLdb\n",
    "\n",
    "mysql = MySQLdb.connect(host=\"mysql\", user=\"root\", passwd=\"mysql\")\n",
    "cur = mysql.cursor()\n",
    "cur.execute(\"CREATE DATABASE IF NOT EXISTS presto\")\n",
    "cur.fetchall()\n",
    "cur.execute(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS presto.events (event LONGTEXT)\n",
    "    CHARSET utf8mb4 ENGINE=InnoDB\n",
    "\"\"\")\n",
    "cur.fetchall()\n",
    "cur.execute(\"DESC presto.events\")\n",
    "for row in cur.fetchall():\n",
    "    print(\"{table}: [{props}]\".format(\n",
    "        table=row[0],\n",
    "        props=', '.join(str(i) for i in row[1:])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data in MySQL\n",
    "\n",
    "Let's now load data from [GH Archive](http://www.gharchive.org/) into MySQL and MongoDB.\n",
    "Each file from GH Archive contains lines of JSON structs that represent events from the public GitHub timeline, for example repository creation or code push.\n",
    "\n",
    "Now that the table is create in MySQL, we can insert rows with Presto by using the existing `conn` object created above. You can open http://localhost:8080 to see the execution Presto queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import io\n",
    "import json\n",
    "import re\n",
    "import requests\n",
    "\n",
    "# Load events happening between 4-5pm.\n",
    "# Feel free to load more hours or more days.\n",
    "# We limit the dataset to one hour here to not overload\n",
    "# the machine that will run the queries as this tutorial\n",
    "# is expected to run on a laptop.\n",
    "# It is going to take some time. For the demo, i pre-loaded\n",
    "# the data with the mysql client to avoid the overhead of creating\n",
    "# Python objects.\n",
    "zdata = requests.get(\"https://data.gharchive.org/2015-04-28-16.json.gz\")\n",
    "data = gzip.decompress(zdata.content)\n",
    "rows = []\n",
    "\n",
    "# load ``ROW_COUNT`` rows. Feel free to set a greater value if it\n",
    "# works well in your environment. Using a small value on purpose\n",
    "# to avoid loading data for a long time.\n",
    "ROW_COUNT = 1000\n",
    "cur = conn.cursor()\n",
    "for n, line in enumerate(io.BytesIO(data)):\n",
    "    row = line.strip().decode('utf8')\n",
    "    sql = \"INSERT INTO mysql.presto.events (event) VALUES ('{}')\".format(row.replace(\"'\", \"''\"))\n",
    "    cur.execute(sql)\n",
    "    cur.fetchall()\n",
    "    if n == ROW_COUNT - 1:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur = conn.cursor()\n",
    "cur.execute(\"SELECT json_extract(json_parse(event), '$.type') FROM mysql.presto.events TABLESAMPLE BERNOULLI (1) LIMIT 1\")\n",
    "cur.fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur = conn.cursor()\n",
    "cur.execute(\"\"\"\n",
    "    SELECT ev_type, repo_name, count(*) FROM (\n",
    "      SELECT\n",
    "        TRY(json_extract_scalar(ev, '$.repo.name')) as repo_name,\n",
    "        TRY(json_extract_scalar(ev, '$.type')) as ev_type FROM (\n",
    "          SELECT try(json_parse(event)) as ev FROM mysql.presto.events))\n",
    "    WHERE repo_name is not null and ev_type = 'PushEvent'\n",
    "    GROUP BY ev_type, repo_name\n",
    "    ORDER BY 3 DESC\n",
    "    LIMIT 10\n",
    "\"\"\")\n",
    "rows = cur.fetchall()\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "print(rows)\n",
    "df = pd.DataFrame(sorted(rows, key=lambda x: x[2], reverse=True))\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur = conn.cursor()\n",
    "cur.execute(\"CREATE TABLE mongodb.events.all AS SELECT * FROM mysql.presto.events\")\n",
    "cur.fetchall()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
